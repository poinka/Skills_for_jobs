{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extract Data from Wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.12.0)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (44.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.10.0)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (25.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (2.3.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (0.11.0)\n",
      "Requirement already satisfied: packaging in /Users/polinakorobeinikova/Library/Python/3.12/lib/python/site-packages (from scrapy) (24.1)\n",
      "Requirement already satisfied: tldextract in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (5.1.3)\n",
      "Requirement already satisfied: lxml>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (5.3.1)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyOpenSSL>=22.0.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
      "Requirement already satisfied: pyasn1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: automat>=24.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tldextract->scrapy) (3.17.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /Users/polinakorobeinikova/IU/Data Wrangling and Visualisation/Assignment 1/1 Part/highest_grossing_films\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject ощиы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Spider \n",
    "Create *parse_jobs.py* for parsing data in the directory *jobs/jobs/spiders* with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import uuid\n",
    "from urllib.parse import urljoin\n",
    "from scrapy.http import Request\n",
    "\n",
    "class HabrSpider(scrapy.Spider):\n",
    "    name = 'parse_jobs'\n",
    "    allowed_domains = ['career.habr.com']\n",
    "    start_urls = [\n",
    "        'https://career.habr.com/vacancies?s[]=2&s[]=3&s[]=4&s[]=82&s[]=72&s[]=5&s[]=75&s[]=6&s[]=1&s[]=77&s[]=7&s[]=83&s[]=84&s[]=73&s[]=8&s[]=85&s[]=86&s[]=188&s[]=178&s[]=106&s[]=78&s[]=21&s[]=172&s[]=174&s[]=79&s[]=173&s[]=80&s[]=176&s[]=81&s[]=118&s[]=182&s[]=44&s[]=125&s[]=177&s[]=175&s[]=126&s[]=98&s[]=41&s[]=42&s[]=99&s[]=168&s[]=43&s[]=76&s[]=96&s[]=97&s[]=95&s[]=100&s[]=133&s[]=111&type=all'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "        'FEEDS': {\n",
    "            'habr_jobs.json': {\n",
    "                'format': 'json',\n",
    "                'encoding': 'utf8',\n",
    "                'store_empty': False,\n",
    "                'indent': 4,\n",
    "            }\n",
    "        },\n",
    "        'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "        'DEFAULT_REQUEST_HEADERS': {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "        },\n",
    "        'RETRY_TIMES': 3,\n",
    "        'RETRY_HTTP_CODES': [403, 429],\n",
    "        'ROBOTSTXT_OBEY': False,  # Check Habr's terms\n",
    "    }\n",
    "    job_count = 0\n",
    "    max_jobs = 1500\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield Request(url, callback=self.parse, meta={'dont_redirect': False})\n",
    "\n",
    "    def parse(self, response):\n",
    "        if response.status in [403, 429]:\n",
    "            self.logger.error(f\"Received {response.status} on {response.url}. Check bot detection.\")\n",
    "            return\n",
    "\n",
    "        # Extract job cards\n",
    "        job_cards = response.css('div.vacancy-card')\n",
    "        if not job_cards:\n",
    "            self.logger.warning(f\"No job cards found on {response.url}. Check HTML structure.\")\n",
    "            self.logger.debug(f\"Response body: {response.text[:1000]}\")\n",
    "\n",
    "        for job in job_cards:\n",
    "            if self.job_count >= self.max_jobs:\n",
    "                return\n",
    "\n",
    "            job_link = job.css('a.vacancy-card__title-link::attr(href)').get()\n",
    "            if job_link:\n",
    "                self.job_count += 1\n",
    "                full_link = urljoin('https://career.habr.com', job_link)\n",
    "                job_data = self.extract_job_data(job)\n",
    "                yield Request(full_link, callback=self.parse_job, meta={'job_data': job_data})\n",
    "\n",
    "        # Follow pagination\n",
    "        next_page = response.css('a.next_page::attr(href)').get()\n",
    "        if next_page and self.job_count < self.max_jobs:\n",
    "            next_page_url = urljoin('https://career.habr.com', next_page)\n",
    "            yield Request(next_page_url, callback=self.parse, meta={'dont_redirect': False})\n",
    "\n",
    "    def extract_job_data(self, job):\n",
    "        \"\"\"Extract data from job card, including skills.\"\"\"\n",
    "        company = job.css('div.vacancy-card__company-title a::text').get(default='N/A').strip()\n",
    "        company_link = job.css('div.vacancy-card__company-title a::attr(href)').get(default='N/A')\n",
    "        company_link = urljoin('https://career.habr.com', company_link) if company_link != 'N/A' else 'N/A'\n",
    "        title = job.css('a.vacancy-card__title-link::text').get(default='N/A').strip()\n",
    "        work_format = job.css('div.vacancy-card__meta span::text').get(default='N/A').strip()\n",
    "        skills = job.css('div.vacancy-card__skills a.link-comp::text').getall()\n",
    "        skills = [skill.strip() for skill in skills if skill.strip()]\n",
    "\n",
    "        return {\n",
    "            'company': company,\n",
    "            'company_link': company_link,\n",
    "            'title': title,\n",
    "            'work_format': work_format,\n",
    "            'skills': skills\n",
    "        }\n",
    "\n",
    "    def parse_job(self, response):\n",
    "        job_data = response.meta['job_data']\n",
    "\n",
    "        yield {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'title': job_data['title'],\n",
    "            'company': job_data['company'],\n",
    "            'url': response.url,\n",
    "            'company_link': job_data['company_link'],\n",
    "            'work_format': job_data['work_format'],\n",
    "            'skills': job_data['skills']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the Spider \n",
    "Run the following command in folder *highest_grossing_films* in your terminal to execute the Scrapy spider:\n",
    "\n",
    "    scrapy crawl parse_jobs -o jobs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean data\n",
    "1. Extract job specialization and level\n",
    "2. Divide all vacancies to *Software Development*, *Analytics*, *Information Security*, *Artificial Intelligence*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorization mappings\n",
    "software_dev = [\n",
    "    'Бэкенд разработчик', 'Backend Developer', 'Фронтенд разработчик', 'Frontend Developer',\n",
    "    'Фулстек разработчик', 'Fullstack Developer', 'Веб-разработчик', 'Web Developer',\n",
    "    'Разработчик приложений', 'Application Developer', 'Разработчик мобильных приложений', 'Mobile Application Developer',\n",
    "    'Релиз менеджер', 'Release Manager', 'Разработчик игр', 'Game Developer',\n",
    "    'Десктоп разработчик', 'Software Developer', 'Разработчик баз данных', 'Database Developer',\n",
    "    'Инженер встраиваемых систем', 'Embedded Software Engineer', 'HTML-верстальщик', 'HTML Coding',\n",
    "    'Программист 1С', '1C Developer', 'Архитектор программного обеспечения', 'Software Architect',\n",
    "    'Системный инженер', 'System Software Engineer', 'ERP-программист', 'ERP Developer',\n",
    "    'Архитектор баз данных', 'Database Architect', 'Инженер электронных устройств', 'Hardware Engineer',\n",
    "    'Архитектор 1С', '1C Architect'\n",
    "]\n",
    "\n",
    "analytics = [\n",
    "    'Аналитик мобильных приложений', 'Mobile Analyst', 'Системный аналитик', 'Systems Analyst',\n",
    "    'Бизнес-аналитик', 'Business Analyst', 'Гейм-аналитик', 'Game Analyst',\n",
    "    'UX-аналитик', 'UX Analyst', 'Аналитик по данным', 'Data Analyst',\n",
    "    'Инженер по данным', 'Data Engineer', 'Программный аналитик', 'Software Analyst',\n",
    "    'Продуктовый аналитик', 'Product Analyst', 'BI-разработчик', 'BI Developer',\n",
    "    'Веб-аналитик', 'Web Analyst', 'Аналитик 1С', '1С Analyst'\n",
    "]\n",
    "\n",
    "info_security = [\n",
    "    'Пентестер', 'Pentester', 'Администратор защиты', 'Security Administrator',\n",
    "    'Аналитик SOC', 'SOC Analyst', 'Специалист по информационной безопасности', 'Information Security Specialist',\n",
    "    'Специалист по реверс-инжинирингу', 'Reverse Engineer', 'AppSec-инженер', 'AppSec-Engineer',\n",
    "    'Инженер по безопасности', 'Security Engineer', 'NLP-инженер', 'NLP-Engineer',\n",
    "    'Антифрод аналитик', 'Antifraud Analyst', 'Архитектор информационной безопасности', 'Information Security Architect'\n",
    "]\n",
    "\n",
    "ai = [\n",
    "    'Ученый по данным', 'Data Scientist', 'ML разработчик', 'ML Engineer',\n",
    "    'Инженер по компьютерному зрению', 'Computer Vision Engineer'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize specialization\n",
    "def categorize_job(specialization):\n",
    "    spec = specialization.strip() if specialization else 'Other'\n",
    "    if spec in software_dev:\n",
    "        return 'Software Development'\n",
    "    elif spec in analytics:\n",
    "        return 'Analytics'\n",
    "    elif spec in info_security:\n",
    "        return 'Information Security'\n",
    "    elif spec in ai:\n",
    "        return 'Artificial Intelligence'\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"jobs.json\" \n",
    "df = pd.read_json(file_path)\n",
    "\n",
    "# Extract job specialization and level\n",
    "df['specialization'] = df['skills'].apply(lambda x: x[0] if x and len(x) > 0 else 'Other')\n",
    "df['level'] = df['skills'].apply(lambda x: x[1] if x[1] in [\"Средний (Middle)\", \"Старший (Senior)\", \"Ведущий (Lead)\", \"Младший (Junior)\", 'Стажёр (Intern)'] and len(x) > 1 else 'Not specified')\n",
    "df['skills'] = df['skills'].apply(lambda x: x[2:] if x[1] in [\"Средний (Middle)\", \"Старший (Senior)\", \"Ведущий (Lead)\", \"Младший (Junior)\", 'Стажёр (Intern)'] and len(x) > 2 else x[1:])\n",
    "\n",
    "# Categorize vacancies\n",
    "df['category'] = df['specialization'].apply(categorize_job)\n",
    "\n",
    "# Save the cleaned dataset as a new JSON file\n",
    "cleaned_file_path = \"cleaned_jobs.json\"\n",
    "df.to_json(cleaned_file_path, orient=\"records\", indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when data cleaning is done, we can proceed to Database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
